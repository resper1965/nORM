# AI Gateway & Vercel AI SDK

DocumentaÃ§Ã£o completa da integraÃ§Ã£o de IA no nORM usando AI Gateway e Vercel AI SDK.

---

## ğŸ¯ O que Ã© o AI Gateway?

O AI Gateway Ã© uma camada de abstraÃ§Ã£o inteligente que fica entre sua aplicaÃ§Ã£o e as APIs de IA (OpenAI, etc). Ele fornece:

- âœ… **Cache inteligente** - Economize em requisiÃ§Ãµes repetidas
- âœ… **Rate limiting** - Controle de uso automÃ¡tico
- âœ… **Fallback automÃ¡tico** - GPT-4 â†’ GPT-4-turbo â†’ GPT-3.5
- âœ… **Cost tracking** - Rastreamento automÃ¡tico de custos
- âœ… **Retry com backoff** - ResiliÃªncia em falhas
- âœ… **Streaming support** - Respostas em tempo real

---

## ğŸ“š Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  AI Gateway     â”‚
â”‚  - Cache        â”‚
â”‚  - Rate Limit   â”‚
â”‚  - Fallback     â”‚
â”‚  - Cost Track   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   OpenAI API    â”‚
â”‚  - GPT-4        â”‚
â”‚  - GPT-3.5      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Como Usar

### 1. Chamada BÃ¡sica (sem streaming)

```typescript
import { callAIGateway } from '@/lib/ai/gateway'

const response = await callAIGateway(
  [
    { role: 'system', content: 'VocÃª Ã© um assistente Ãºtil' },
    { role: 'user', content: 'OlÃ¡!' }
  ],
  {
    model: 'gpt-4',
    temperature: 0.7,
    maxTokens: 1000,
    useCache: true,        // Habilita cache
    enableFallback: true,  // Habilita fallback automÃ¡tico
  }
)

console.log(response.content)    // Resposta da IA
console.log(response.model)      // Modelo usado (pode ser fallback)
console.log(response.cost)       // Custo da chamada em USD
console.log(response.cached)     // Se veio do cache
```

### 2. Streaming (tempo real)

```typescript
import { streamAIGateway } from '@/lib/ai/gateway'

const response = await streamAIGateway(
  [
    { role: 'user', content: 'Escreva um artigo sobre SEO' }
  ],
  {
    model: 'gpt-4',
    temperature: 0.7,
  },
  (chunk) => {
    // Callback chamado para cada chunk recebido
    console.log(chunk)
    // Atualize a UI aqui
  }
)

console.log(response.content) // ConteÃºdo completo no final
```

### 3. Hook React (Client-side)

```tsx
'use client'

import { useContentGeneration } from '@/lib/hooks/use-ai-stream'

function MyComponent() {
  const { content, generate, isLoading } = useContentGeneration()

  const handleGenerate = async () => {
    await generate({
      topic: 'SEO Tips',
      keywords: ['seo', 'optimization'],
      tone: 'professional',
      length: 'medium'
    })
  }

  return (
    <div>
      <button onClick={handleGenerate} disabled={isLoading}>
        Gerar
      </button>
      <div>{content}</div> {/* Atualizado em tempo real! */}
    </div>
  )
}
```

---

## âš™ï¸ ConfiguraÃ§Ã£o

### Environment Variables

```bash
# ObrigatÃ³rio
OPENAI_API_KEY=sk-...

# Opcional (com valores padrÃ£o)
MAX_OPENAI_REQUESTS_PER_HOUR=100  # Rate limit
```

### Config Options

```typescript
interface AIGatewayConfig {
  model: 'gpt-4' | 'gpt-4-turbo' | 'gpt-3.5-turbo'
  temperature?: number      // 0.0 - 2.0, default: 0.7
  maxTokens?: number       // Max tokens, default: 2000
  useCache?: boolean       // Enable cache, default: true
  cacheTTL?: number        // Cache TTL em ms, default: 1h
  enableFallback?: boolean // Enable fallback, default: true
  retries?: number         // Retry attempts, default: 3
}
```

---

## ğŸ’° Cost Tracking

O AI Gateway rastreia automaticamente os custos de cada chamada:

```typescript
const response = await callAIGateway(messages, config)

console.log(response.cost) // 0.045 (em USD)
console.log(response.usage)
// {
//   promptTokens: 100,
//   completionTokens: 500,
//   totalTokens: 600
// }
```

Os custos sÃ£o automaticamente enviados para o **Cost Tracker**:

```typescript
import { getTotalCosts } from '@/lib/monitoring/cost-tracker'

const costs = await getTotalCosts('this_month')
console.log(costs.total)        // Total gasto
console.log(costs.by_category)  // Por modelo/API
```

---

## ğŸ”„ Fallback Chain

Se um modelo falhar, o AI Gateway tenta automaticamente o prÃ³ximo:

```
GPT-4 (falhou)
  â†“
GPT-4-turbo (tentando...)
  â†“
GPT-3.5-turbo (tentando...)
  â†“
Error (todos falharam)
```

**Exemplo**:

```typescript
const response = await callAIGateway(
  messages,
  {
    model: 'gpt-4',
    enableFallback: true  // âœ… Ativa fallback
  }
)

// Se GPT-4 falhar, automaticamente usa GPT-4-turbo
// Se GPT-4-turbo falhar, usa GPT-3.5-turbo
// VocÃª recebe o resultado sem se preocupar!

console.log(response.model) // Mostra qual modelo foi usado
```

---

## ğŸ“¦ Cache

O cache economiza **tempo** e **dinheiro** ao reutilizar respostas:

### Como funciona

```typescript
// Primeira chamada - vai para OpenAI
const response1 = await callAIGateway(messages, { useCache: true })
console.log(response1.cached) // false
console.log(response1.cost)    // 0.045

// Segunda chamada IDÃŠNTICA - vem do cache
const response2 = await callAIGateway(messages, { useCache: true })
console.log(response2.cached) // true âœ…
console.log(response2.cost)    // 0.045 (mesmo valor)
// Mas vocÃª nÃ£o foi cobrado novamente!
```

### TTL (Time To Live)

Por padrÃ£o, cache expira em **1 hora**. VocÃª pode customizar:

```typescript
await callAIGateway(messages, {
  useCache: true,
  cacheTTL: 30 * 60 * 1000  // 30 minutos
})
```

### Clear Cache

```typescript
import { clearAICache } from '@/lib/ai/gateway'

clearAICache() // Limpa todo o cache
```

---

## ğŸš¦ Rate Limiting

Protege contra excesso de uso:

```typescript
// Configurado via env
MAX_OPENAI_REQUESTS_PER_HOUR=100

// Se exceder o limite:
const response = await callAIGateway(messages)
// Error: Rate limit exceeded. Please try again later.
```

---

## ğŸ” Retry com Exponential Backoff

Se a API falhar temporariamente, o Gateway tenta novamente:

```
Tentativa 1 - Falhou
  â†“ aguarda 1s
Tentativa 2 - Falhou
  â†“ aguarda 2s
Tentativa 3 - Falhou
  â†“ aguarda 4s
Erro final
```

```typescript
await callAIGateway(messages, {
  retries: 3  // NÃºmero de tentativas
})
```

**Erros que NÃƒO retentam** (fail fast):
- `invalid_request_error`
- `insufficient_quota`

---

## ğŸ“Š Logs & Monitoring

Todos os eventos sÃ£o logados:

```typescript
logger.info('AI Gateway success', {
  model: 'gpt-4',
  tokens: 600,
  cost: 0.045,
})

logger.warn('AI Gateway fallback', {
  from: 'gpt-4',
  to: 'gpt-3.5-turbo'
})

logger.error('AI Gateway error', { error })
```

---

## ğŸ¨ Exemplos PrÃ¡ticos

### Sentiment Analysis

```typescript
import { analyzeSentiment } from '@/lib/ai/sentiment'

const result = await analyzeSentiment('Este produto Ã© incrÃ­vel!')

console.log(result.sentiment)   // 'positive'
console.log(result.score)       // 0.9
console.log(result.confidence)  // 0.95
```

> **Nota**: `analyzeSentiment` usa internamente o AI Gateway com cache habilitado e GPT-3.5-turbo (mais barato para sentiment).

### Content Generation (Streaming)

```tsx
import { ContentGeneratorStream } from '@/components/ai/content-generator-stream'

export default function Page() {
  return <ContentGeneratorStream />
}
```

---

## ğŸ”’ SeguranÃ§a

1. **API Keys**: Nunca expor no client-side
   ```typescript
   // âŒ ERRADO - client-side
   const apiKey = 'sk-...'

   // âœ… CORRETO - server-side only
   const apiKey = process.env.OPENAI_API_KEY
   ```

2. **Rate Limiting**: Sempre habilitado
3. **Validation**: Todas as entradas sÃ£o validadas

---

## ğŸ“ˆ Performance

### Benchmarks

| OperaÃ§Ã£o | Sem Gateway | Com Gateway |
|----------|-------------|-------------|
| Primeira chamada | ~2s | ~2s |
| Chamada repetida (cache) | ~2s | **~50ms** âš¡ |
| Falha + fallback | Error | ~3s (auto recovery) |

### OtimizaÃ§Ãµes

1. **Use cache** para requests repetitivos
2. **GPT-3.5-turbo** para tarefas simples (10x mais barato)
3. **Streaming** para melhor UX
4. **Fallback** para maior resiliÃªncia

---

## ğŸ› Troubleshooting

### Cache nÃ£o estÃ¡ funcionando

```typescript
// Verifique se useCache estÃ¡ true
const response = await callAIGateway(messages, {
  useCache: true  // âœ…
})
```

### Rate limit muito restritivo

```bash
# Aumente o limite no .env
MAX_OPENAI_REQUESTS_PER_HOUR=200
```

### Fallback nÃ£o ativa

```typescript
// Verifique se enableFallback estÃ¡ true
const response = await callAIGateway(messages, {
  enableFallback: true  // âœ…
})
```

---

## ğŸ“š ReferÃªncias

- [Vercel AI SDK Docs](https://sdk.vercel.ai/docs)
- [OpenAI API Reference](https://platform.openai.com/docs/api-reference)
- [Cost Tracker](../lib/monitoring/cost-tracker.ts)
- [Performance Monitor](../lib/utils/performance.ts)
